1. <b>crawler.sh</b>                  to crawl from gau,hakrawker,wayback.

2. <b>filter-urls.sh</b>              to get crawl result from specific domain as crawler crawls everything.

3. <b>url-encode</b>                  to encode special character to curl can check if its alive or dead.

4. <b>crawl-stat-check</b>            to check if a url exits or dead.

5. <b>categorize_urls.sh</b>          to filter url using response code.

6. <b>kxss.sh</b>                     to check any reflection.

7. <b>kxss_in.sh</b>                  to check any reflection.
