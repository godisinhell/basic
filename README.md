<<<<<<< HEAD
Run crawler.sh                  to crawl from gau,hakrawker,wayback.

Run filter-urls.sh              to get crawl result from specific domain as crawler crawls everything.

Run url-encode                  to encode special character to curl can check if its alive or dead.

Run crawl-stat-check            to check if a url exits or dead.

Run categorize_urls.sh          to filter url using response code.

Run kxss.sh                     to check any reflection.

Run kxss_in.sh                  to check any reflection.
=======
1. <b>crawler.sh</b>                  to crawl from gau,hakrawker,wayback.

2. <b>filter-urls.sh</b>              to get crawl result from specific domain as crawler crawls everything.

3. <b>url-encode</b>                  to encode special character to curl can check if its alive or dead.

4. <b>crawl-stat-check</b>            to check if a url exits or dead.

5. <b>categorize_urls.sh</b>          to filter url using response code.

6. <b>kxss.sh</b>                     to check any reflection.

7. <b>kxss_in.sh</b>                  to check any reflection.
>>>>>>> a9c6d7915096d87dab17c25294f6c2a9f6e1b34b
